{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Year Project - Project 3: Natural Language Processing\n",
    "***\n",
    "#### Group G: Christina Aftzidis, Germán Buttiero, Hubert Wójcik, Laurids Holme Pedersen, Paula Menshikoff\n",
    "Submission: 03.07.2022\n",
    "***\n",
    "\n",
    "This notebook contains the process of training a classifier with (a) tweets containing hate speech and (b) tweets with stance on a certain topic, and classifying them into categories. \n",
    "\n",
    "The initial data was obtained from the [TweetEval](https://github.com/cardiffnlp/tweeteval) GitHub repository. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running this notebook\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries and further imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports\n",
    "import numpy as np\n",
    "import re \n",
    "import pandas as pd\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score \n",
    "\n",
    "#imports for data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing our classifiers from our script\n",
    "from scripts.classifiers import classifier #check if this works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports that need to be installed in the environment the code is run in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the spacy pipeline pretrained on the english language\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the list of stopwords\n",
    "!python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "***\n",
    "\n",
    "implement a tokenizer to split inputs into tokens\n",
    "\n",
    "tokenizer as script that returns one output line for every input line in the input\n",
    "\n",
    "compare to the nltk tweet tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterising the data\n",
    "\n",
    "***\n",
    "\n",
    "- corpus size, vocab size, type/token ratio\n",
    "- most frequent tokens?\n",
    "- types of tokens that only occur once, or 2 or 3 times?\n",
    "- any noticeable differences between two datasets?\n",
    "- corpus statistics consistent with zipf's law? (plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual annotation and Inter-Annotator Agreement\n",
    "\n",
    "*** \n",
    "\n",
    "report on the inter annotator agreement, including the agreement with the published labels and discuss what phenomena in the data caused the biggest problems for inter-annotator agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "***\n",
    "\n",
    "run the one with the highest accuracy on the validation set with the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa82987a71ca0339a52998cd22613b0c002bda8349fa32a0cb67ceec3936a2bf"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
